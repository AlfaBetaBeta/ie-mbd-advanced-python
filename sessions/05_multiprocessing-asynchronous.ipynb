{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![IE](../img/ie.png)\n",
    "\n",
    "# Sessions 9 & 10: Multi-process and asynchronous programming\n",
    "\n",
    "### Juan Luis Cano Rodríguez <jcano@faculty.ie.edu> - Master in Business Analytics and Big Data (2019-04-24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First rule of performance analysis\n",
    "\n",
    "> \"We should forget about small efficiencies, say about 97% of the time: premature optimization is the root of all evil.\"\n",
    ">\n",
    "> — Donald E. Knuth\n",
    "\n",
    "> **premature** (adj.) pre·ma·ture : happening, arriving, existing, or performed before the proper, usual, or intended time\n",
    ">\n",
    "> Merriam-Webster dictionary\n",
    "\n",
    "There are several techniques to make slow programs go faster. However, no matter what we do, the first step is to **analyze** _where does the slowdown come from_.\n",
    "\n",
    "Among the most straight-forward tools to analyze performance in Python are cProfile and line_profiler, and they are very easy to use from the notebook. cProfile is part of the standard library, whereas line_profiler must be installed with pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cython in /home/juanlu/.miniconda36/envs/mbd19/lib/python3.7/site-packages (0.29.2)\n",
      "Collecting https://github.com/rkern/line_profiler/archive/master.zip\n",
      "  Downloading https://github.com/rkern/line_profiler/archive/master.zip\n",
      "\u001b[K     / 245kB 258kB/s\n",
      "Requirement already satisfied (use --upgrade to upgrade): line-profiler==2.1.1 from https://github.com/rkern/line_profiler/archive/master.zip in /home/juanlu/.miniconda36/envs/mbd19/lib/python3.7/site-packages\n",
      "Requirement already satisfied: IPython>=0.13 in /home/juanlu/.miniconda36/envs/mbd19/lib/python3.7/site-packages (from line-profiler==2.1.1) (6.2.1)\n",
      "Requirement already satisfied: decorator in /home/juanlu/.miniconda36/envs/mbd19/lib/python3.7/site-packages (from IPython>=0.13->line-profiler==2.1.1) (4.3.0)\n",
      "Requirement already satisfied: pygments in /home/juanlu/.miniconda36/envs/mbd19/lib/python3.7/site-packages (from IPython>=0.13->line-profiler==2.1.1) (2.3.1)\n",
      "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /home/juanlu/.miniconda36/envs/mbd19/lib/python3.7/site-packages (from IPython>=0.13->line-profiler==2.1.1) (1.0.15)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /home/juanlu/.miniconda36/envs/mbd19/lib/python3.7/site-packages (from IPython>=0.13->line-profiler==2.1.1) (4.6.0)\n",
      "Requirement already satisfied: simplegeneric>0.8 in /home/juanlu/.miniconda36/envs/mbd19/lib/python3.7/site-packages (from IPython>=0.13->line-profiler==2.1.1) (0.8.1)\n",
      "Requirement already satisfied: traitlets>=4.2 in /home/juanlu/.miniconda36/envs/mbd19/lib/python3.7/site-packages (from IPython>=0.13->line-profiler==2.1.1) (4.3.2)\n",
      "Requirement already satisfied: jedi>=0.10 in /home/juanlu/.miniconda36/envs/mbd19/lib/python3.7/site-packages (from IPython>=0.13->line-profiler==2.1.1) (0.13.2)\n",
      "Requirement already satisfied: setuptools>=18.5 in /home/juanlu/.miniconda36/envs/mbd19/lib/python3.7/site-packages (from IPython>=0.13->line-profiler==2.1.1) (40.6.3)\n",
      "Requirement already satisfied: pickleshare in /home/juanlu/.miniconda36/envs/mbd19/lib/python3.7/site-packages (from IPython>=0.13->line-profiler==2.1.1) (0.7.5)\n",
      "Requirement already satisfied: wcwidth in /home/juanlu/.miniconda36/envs/mbd19/lib/python3.7/site-packages (from prompt-toolkit<2.0.0,>=1.0.4->IPython>=0.13->line-profiler==2.1.1) (0.1.7)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/juanlu/.miniconda36/envs/mbd19/lib/python3.7/site-packages (from prompt-toolkit<2.0.0,>=1.0.4->IPython>=0.13->line-profiler==2.1.1) (1.12.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/juanlu/.miniconda36/envs/mbd19/lib/python3.7/site-packages (from pexpect; sys_platform != \"win32\"->IPython>=0.13->line-profiler==2.1.1) (0.6.0)\n",
      "Requirement already satisfied: ipython-genutils in /home/juanlu/.miniconda36/envs/mbd19/lib/python3.7/site-packages (from traitlets>=4.2->IPython>=0.13->line-profiler==2.1.1) (0.2.0)\n",
      "Requirement already satisfied: parso>=0.3.0 in /home/juanlu/.miniconda36/envs/mbd19/lib/python3.7/site-packages (from jedi>=0.10->IPython>=0.13->line-profiler==2.1.1) (0.3.1)\n",
      "Building wheels for collected packages: line-profiler\n",
      "  Running setup.py bdist_wheel for line-profiler ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-4vx_2zg7/wheels/c0/45/5c/273eebba2150941af640a9801c65dec439f35f747e694aeb52\n",
      "Successfully built line-profiler\n"
     ]
    }
   ],
   "source": [
    "#!pip install line_profiler  # Python <= 3.6\n",
    "!pip install cython  # Required to install line_profiler from source\n",
    "!pip install https://github.com/rkern/line_profiler/archive/master.zip  # Required in Python 3.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a simple example: a function to estimate the value of $\\pi$ using a Monte Carlo simulation:\n",
    "\n",
    "![Monte Carlo pi](../img/monte_carlo_pi.png)\n",
    "\n",
    "(Source: https://towardsdatascience.com/speed-up-jupyter-notebooks-20716cbe2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def estimate_pi(num_sim=1e7):\n",
    "    \"\"\"Estimate pi with monte carlo simulation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    num_sim: int\n",
    "        Number of simulations.\n",
    "\n",
    "    \"\"\"\n",
    "    in_circle = 0\n",
    "    ii = 0\n",
    "    while ii < num_sim:\n",
    "        prec_x = np.random.rand()\n",
    "        prec_y = np.random.rand()\n",
    "\n",
    "        # Let's use pow here instead of **\n",
    "        # to see it in the cProfile report\n",
    "        if pow(prec_x, 2) + pow(prec_y, 2) <= 1:\n",
    "            in_circle += 1  # inside the circle\n",
    "\n",
    "        ii += 1\n",
    "        \n",
    "    return 4 * in_circle / num_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.88\n",
      "3.084\n",
      "3.1552\n",
      "3.14296\n"
     ]
    }
   ],
   "source": [
    "for num_sims in 1e2, 1e3, 1e4, 1e5:\n",
    "    print(estimate_pi(num_sims))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running it through `cProfile` will tell us which functions or methods are called more. For that, we have to write `%prun` in front of the executable code, adding `-s cumtime` to sort by cumulative time. Notice that this will only work in IPython and Jupyter notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    }
   ],
   "source": [
    "%prun -s cumtime estimate_pi(1e6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This opens a separate panel with information similar to this:\n",
    "\n",
    "```\n",
    "         4000004 function calls in 3.821 seconds\n",
    "\n",
    "   Ordered by: cumulative time\n",
    "\n",
    "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
    "        1    0.000    0.000    3.821    3.821 {built-in method builtins.exec}\n",
    "        1    0.000    0.000    3.821    3.821 <string>:1(<module>)\n",
    "        1    2.048    2.048    3.821    3.821 <ipython-input-87-d37e0e33fe4b>:4(estimate_pi)\n",
    "  2000000    1.296    0.000    1.296    0.000 {method 'rand' of 'mtrand.RandomState' objects}\n",
    "  2000000    0.478    0.000    0.478    0.000 {built-in method builtins.pow}\n",
    "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes the information shown by `cProfile` gives a useful hint because it's a summary of the whole execution. However, other times it's better to profile the program line by line, and visually identifying the hotspots. For that, we have to load the `line_profiler` IPython extension and run the code with `%lprun`, adding `-f <my_function` for all the functions I want to display in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun -f estimate_pi estimate_pi(1e6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This again opens a separate panel with information similar to this:\n",
    "\n",
    "```\n",
    "Timer unit: 1e-06 s\n",
    "\n",
    "Total time: 8.75213 s\n",
    "File: <ipython-input-2-d37e0e33fe4b>\n",
    "Function: estimate_pi at line 4\n",
    "\n",
    "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
    "==============================================================\n",
    "     4                                           def estimate_pi(num_sim=1e7):\n",
    "     5                                               \"\"\"Estimate pi with monte carlo simulation.\n",
    "     6                                               \n",
    "     7                                               Parameters\n",
    "     8                                               ----------\n",
    "     9                                               num_sim: int\n",
    "    10                                                   Number of simulations.\n",
    "    11                                           \n",
    "    12                                               \"\"\"\n",
    "    13         1          9.0      9.0      0.0      in_circle = 0\n",
    "    14         1          6.0      6.0      0.0      ii = 0\n",
    "    15   1000001    1074014.0      1.1     12.3      while ii < num_sim:\n",
    "    16   1000000    1916723.0      1.9     21.9          prec_x = np.random.rand()\n",
    "    17   1000000    1812770.0      1.8     20.7          prec_y = np.random.rand()\n",
    "    18                                           \n",
    "    19                                                   # Let's use pow here instead of **\n",
    "    20                                                   # to see it in the cProfile report\n",
    "    21   1000000    1947163.0      1.9     22.2          if pow(prec_x, 2) + pow(prec_y, 2) <= 1:\n",
    "    22    785268     897605.0      1.1     10.3              in_circle += 1  # inside the circle\n",
    "    23                                           \n",
    "    24   1000000    1103842.0      1.1     12.6          ii += 1\n",
    "    25                                                   \n",
    "    26         1          3.0      3.0      0.0      return 4 * in_circle / num_sim\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of parallelism\n",
    "\n",
    "In Python there are two parallelism models:\n",
    "\n",
    "* **Multithreading**: There is one single process that can have several execution _threads_ at the same time. The (theoretical) advantage is that all these threads share the same memory and are lightweight.\n",
    "* **Multiprocessing**: Several processes are launched at the same time. The disadvantage is that they have more overhead and it's more difficult to share data between them.\n",
    "\n",
    "It seems clear that multithreading would be the obvious choice, but in Python it's not the case.\n",
    "\n",
    "We can broadly classify computer programs in two groups:\n",
    "\n",
    "* **CPU-bound**: The bottleneck is the CPU. The faster the CPU, the faster the program will complete. Basically any calculation the computer does with data that resides in RAM.\n",
    "* **I/O-bound**: The bottleneck is the input/output, either from disk, from the network, or any other sources. The faster the I/O, the faster the program will complete. This includes downloading data from the Internet, reading files from disk, and so forth.\n",
    "\n",
    "The Python canonical implementation (CPython, the one everybody uses) has a limitation called the Global Interpreter Lock (GIL), which means that **only one thread can use the CPU at a time**. Therefore, as a rule of thumb, **multithreading should not be used in Python for CPU-bound code**.\n",
    "\n",
    "For the rest of this course, even though multithreading can be used in Python for I/O-bound code, we will skip it entirely, since it's also more complex to synchronize and can lead to subtle errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiprocessing\n",
    "\n",
    "The number of processes we run in parallel shouldn't be higher than the number of CPUs we have, and can be obtained with the `multiprocessing.cpu_count` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from multiprocessing import cpu_count\n",
    "\n",
    "cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to leverage `multiprocessing` is to create a `Pool` with a pre-defind number of processes that will run in parallel in our computer, and that has convenience methods that allow us to call a function with a sequence of arguments in parallel:\n",
    "\n",
    "<div class=\"alert alert-warning\">Under certain circumstances, directly applying <code>multiprocessing.Pool</code> to some code in the notebook might not work, see <a href=\"https://medium.com/@grvsinghal/speed-up-your-python-code-using-multiprocessing-on-windows-and-jupyter-or-ipython-2714b49d6fac\">this explanation</a>. In that case, it should be enough to move the function we want to parallelize to a <code>.py</code> module, or wrap the execution inside a <code>if __name__ == \"__main__\":</code> block.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "def worker(x):\n",
    "    sleep(1)  # Simulate slow process\n",
    "    return x * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "worker(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 s ± 406 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "[worker(x) for x in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15 s ± 54.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# By creating a context, we make sure\n",
    "# the pool is closed when we exit\n",
    "with Pool(cpu_count()) as p:\n",
    "    results = p.map(worker, range(4))\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's _almost_ four times faster! But why doesn't it take exactly 1 second? The reason is mostly **process overhead**. If the process is very fast, the time it taks to spawn a new process becomes the bottleneck. See what happens if we remove the `sleep`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker(x):\n",
    "    return x * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.04 µs ± 150 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "[worker(x) for x in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159 ms ± 9.02 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# By creating a context, we make sure\n",
    "# the pool is closed when we exit\n",
    "with Pool(cpu_count()) as p:\n",
    "    results = p.map(worker, range(4))\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*It's actually 30 000 times slower!* That gives us an idea of what is the process overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Create a Monte Carlo simulation that leverages `multiprocessing` to accelerate the estimation of $\\pi$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
